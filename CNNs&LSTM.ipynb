{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Import and Prceprocessin"
      ],
      "metadata": {
        "id": "Q8qhSkameuKM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-mN40MBCDNV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file load from google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yskFIaXBck8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dvt = pd.read_csv('YOUR FILE NAME HERE')\n",
        "dvt.head()\n",
        "\n",
        "#preprocessing"
      ],
      "metadata": {
        "id": "DCr_ncMGCPu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "q1VP3BahCrOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess(sentence):\n",
        "    sentence=str(sentence)\n",
        "    korean = re.compile('[\\u3131-\\u3163\\uac00-\\ud7a3]+')\n",
        "    sentence = re.sub(korean, '', sentence)\n",
        "    sentence = sentence.lower()\n",
        "    sentence=sentence.replace('{html}',\"\")\n",
        "    cleanr = re.compile('=#$-+<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', sentence)\n",
        "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
        "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(rem_num)\n",
        "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
        "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
        "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "dvt['sentence']=dvt['opinion'].map(lambda s:preprocess(s))"
      ],
      "metadata": {
        "id": "eosdMzudCg0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "ylBK-jL3Cy_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "tokenizer = Tokenizer(num_words = vocab_size)\n",
        "tokenizer.fit_on_texts(dvt['sentence'])\n",
        "sequences = tokenizer.texts_to_sequences(dvt['sentence'])"
      ],
      "metadata": {
        "id": "Zx-dGjFqDuXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_train = int(len(sequences) * 0.8)\n",
        "n_of_test = int(len(sequences) - n_of_train)\n",
        "print('Number of Training data :',n_of_train)\n",
        "print('Number of Test data:',n_of_test)"
      ],
      "metadata": {
        "id": "836iINiNEICR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = sequences\n",
        "print('Max Length : %d' % max(len(l) for l in X_data))\n",
        "print('Mean Length : %f' % (sum(map(len, X_data))/len(X_data)))\n",
        "plt.hist([len(s) for s in X_data], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j49EhltjEJPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of dataset sets to max_len\n",
        "max_len = 163\n",
        "data = pad_sequences(X_data, maxlen = max_len)\n",
        "print(\"Test data(shape): \", data.shape)"
      ],
      "metadata": {
        "id": "eSxYJ3pMEVrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_data = pd.get_dummies(dvt['DVT부위'])\n",
        "\n",
        "X_test = data[n_of_train:]\n",
        "y_test = np.array(y_data[n_of_train:])\n",
        "X_train = data[:n_of_train]\n",
        "y_train = np.array(y_data[:n_of_train])\n",
        "print(\"Training data(shape): \", X_train.shape)\n",
        "print(\"Test data(shape): \", X_test.shape)\n",
        "print(\"Training label(shape): \", y_train.shape)\n",
        "print(\"Test label(shape): \", y_test.shape)"
      ],
      "metadata": {
        "id": "nliv2N25Ebnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1D CNN"
      ],
      "metadata": {
        "id": "0r6A_cEDEtlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout, MaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "uS-Ty1gCEpIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 32))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(32, 5, strides=1, padding='valid', activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "79PiQxGREyFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor = 'val_loss',\n",
        "                   mode = 'min',\n",
        "                   verbose = 1,\n",
        "                   patience = 3)\n",
        "mc = ModelCheckpoint('best_model.keras',\n",
        "                     monitor = 'val_acc',\n",
        "                     mode = 'max',\n",
        "                     verbose = 1,\n",
        "                     save_best_only = True)\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs = 50,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[es, mc])"
      ],
      "metadata": {
        "id": "55gBOWvgFpnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "new_model = tf.keras.models.load_model('best_model.keras')"
      ],
      "metadata": {
        "id": "z8OhFpZ9JpVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (new_model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "ztbkj00aJc_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_test_classes = y_test.argmax(1)\n",
        "y_pred = new_model.predict(X_test)\n",
        "y_pred_classes = y_pred.argmax(1)\n",
        "model_accuracy = f1_score(y_test_classes, y_pred_classes, average='micro')\n",
        "print(f\"f1 score : {model_accuracy}\")"
      ],
      "metadata": {
        "id": "rLJs5XzgJm0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Kernel 1D CNN"
      ],
      "metadata": {
        "id": "7g-YXjhYMpb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, Input, Flatten, Concatenate"
      ],
      "metadata": {
        "id": "H23joUapNXDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_sizes = [2,3,5,7]\n",
        "num_filters = 512\n",
        "drop = 0.5"
      ],
      "metadata": {
        "id": "Yis5oY8mNBdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = Input(shape = (max_len,))\n",
        "z = Embedding(vocab_size, 32)(model_input)\n",
        "\n",
        "conv_blocks = []\n",
        "\n",
        "for sz in filter_sizes:\n",
        "    conv = Conv1D(filters = num_filters,\n",
        "                         kernel_size = sz,\n",
        "                         padding = \"valid\",\n",
        "                         activation = \"relu\",\n",
        "                         strides = 1)(z)\n",
        "    conv = GlobalMaxPooling1D()(conv)\n",
        "    conv = Flatten()(conv)\n",
        "    conv_blocks.append(conv)\n",
        "\n",
        "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
        "z = Dropout(drop)(z)\n",
        "model_output = Dense(5, activation='softmax')(z)\n",
        "\n",
        "model = Model(model_input, model_output)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GyRiXt23NDom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor = 'val_loss',\n",
        "                   mode = 'min',\n",
        "                   verbose = 1,\n",
        "                   patience = 3)\n",
        "mc = ModelCheckpoint('best_model_multi.keras',\n",
        "                     monitor = 'val_acc',\n",
        "                     mode = 'max',\n",
        "                     verbose = 1,\n",
        "                     save_best_only = True)\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs = 50,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[es, mc])"
      ],
      "metadata": {
        "id": "dEmusJ1eNaoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history['acc']) + 1)\n",
        "plt.plot(epochs, history.history['acc'])\n",
        "plt.plot(epochs, history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['train', 'validation'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "epochs = range(1, len(history.history['loss']) + 1)\n",
        "plt.plot(epochs, history.history['loss'])\n",
        "plt.plot(epochs, history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bcXSj92uNnzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('best_model_multi.keras')\n",
        "\n",
        "y_test_classes = y_test.argmax(1)\n",
        "y_pred = new_model.predict(X_test)\n",
        "y_pred_classes = y_pred.argmax(1)\n",
        "model_accuracy = f1_score(y_test_classes, y_pred_classes, average='micro')\n",
        "print(f\"f1 score : {model_accuracy}\")"
      ],
      "metadata": {
        "id": "cr99j26ENrEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Kernel 1D CNN with GLOVE"
      ],
      "metadata": {
        "id": "Cm2T6PgdekcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "id": "pVBITq3xeojp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "embedding_dict = dict()\n",
        "f = open(os.path.join('glove.6B.100d.txt'), encoding='utf-8')\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 100개의 값을 가지는 array로 변환\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "\n",
        "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
      ],
      "metadata": {
        "id": "UXaZfZpffZyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "5lffx4lmgiqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size_glove = len(word_index)"
      ],
      "metadata": {
        "id": "P5l_rf-njIbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((vocab_size_glove, embedding_dim))"
      ],
      "metadata": {
        "id": "MQK23MHsfcyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(embedding_matrix)"
      ],
      "metadata": {
        "id": "-IMMZKJHfdQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_dict.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "fXML8E3vfedO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, Input, Flatten, Concatenate"
      ],
      "metadata": {
        "id": "aOqkIvHnff3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_sizes = [2,3,5]\n",
        "num_filters = 512\n",
        "drop = 0.5"
      ],
      "metadata": {
        "id": "3Nhah7CDfhbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = Input(shape = (max_len,))\n",
        "z = Embedding(vocab_size_glove, embedding_dim, weights=[embedding_matrix],\n",
        "                      input_length=max_len, trainable=False)(model_input)\n",
        "\n",
        "conv_blocks = []\n",
        "\n",
        "for sz in filter_sizes:\n",
        "    conv = Conv1D(filters = num_filters,\n",
        "                         kernel_size = sz,\n",
        "                         padding = \"valid\",\n",
        "                         activation = \"relu\",\n",
        "                         strides = 1)(z)\n",
        "    conv = GlobalMaxPooling1D()(conv)\n",
        "    conv = Flatten()(conv)\n",
        "    conv_blocks.append(conv)\n",
        "\n",
        "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
        "z = Dropout(drop)(z)\n",
        "model_output = Dense(5, activation='softmax')(z)\n",
        "\n",
        "model = Model(model_input, model_output)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "h-w5hxo_fitG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor = 'val_loss',\n",
        "                   mode = 'min',\n",
        "                   verbose = 1,\n",
        "                   patience = 3)\n",
        "mc = ModelCheckpoint('best_model_multi_glove.keras',\n",
        "                     monitor = 'val_acc',\n",
        "                     mode = 'max',\n",
        "                     verbose = 1,\n",
        "                     save_best_only = True)\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs = 50,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[es, mc])"
      ],
      "metadata": {
        "id": "-Pr7KvL2fmg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history['acc']) + 1)\n",
        "plt.plot(epochs, history.history['acc'])\n",
        "plt.plot(epochs, history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['train', 'validation'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "epochs = range(1, len(history.history['loss']) + 1)\n",
        "plt.plot(epochs, history.history['loss'])\n",
        "plt.plot(epochs, history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuNSHpc6fs6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('best_model_multi_glove.keras')\n",
        "\n",
        "y_test_classes = y_test.argmax(1)\n",
        "y_pred = new_model.predict(X_test)\n",
        "y_pred_classes = y_pred.argmax(1)\n",
        "model_accuracy = f1_score(y_test_classes, y_pred_classes, average='micro')\n",
        "print(f\"f1 score : {model_accuracy}\")"
      ],
      "metadata": {
        "id": "AxeHeavEfxye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "hSZimD2GcGd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "iMR3iOQycFrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_len))\n",
        "#model.add(Bidirectional(LSTM(128)))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "metadata": {
        "id": "RDg7iJJ6cPlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss',\n",
        "                   mode='min',\n",
        "                   verbose=1,\n",
        "                   patience=4)\n",
        "mc = ModelCheckpoint('best_model_lstm.keras',\n",
        "                     monitor='val_acc',\n",
        "                     mode='max',\n",
        "                     verbose=1,\n",
        "                     save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs=50,\n",
        "                    callbacks=[es, mc],\n",
        "                    batch_size=60,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "id": "sylfK3cFcTmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history['acc']) + 1)\n",
        "plt.plot(epochs, history.history['acc'])\n",
        "plt.plot(epochs, history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['train', 'validation'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "epochs = range(1, len(history.history['loss']) + 1)\n",
        "plt.plot(epochs, history.history['loss'])\n",
        "plt.plot(epochs, history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dgq6Ik8wc7A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('best_model_lstm.keras')\n",
        "\n",
        "y_test_classes = y_test.argmax(1)\n",
        "y_pred = new_model.predict(X_test)\n",
        "y_pred_classes = y_pred.argmax(1)\n",
        "model_accuracy = f1_score(y_test_classes, y_pred_classes, average='micro')\n",
        "print(f\"f1 score : {model_accuracy}\")"
      ],
      "metadata": {
        "id": "XAwVaGb-dCsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1d_CYrRNeMF2"
      }
    }
  ]
}